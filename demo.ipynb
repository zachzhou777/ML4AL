{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes over all of the major components of this project:\n",
    "1. Simulation: Can be thought of as a black box that takes in a solution and outputs demand coverage. Used to evaluate solutions and generate a dataset.\n",
    "2. Machine learning: A multilayer perceptron (MLP) is trained to predict coverage given a solution.\n",
    "3. Optimization: The MLP is embedded within a MIP which attempts to find the solution that the MLP predicts will have the highest coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ems_data import EMSData, TORONTO_AVG_CALLS_PER_DAY, TORONTO_N_AMBULANCES\n",
    "from simulation import Simulation\n",
    "from neural_network import MLP\n",
    "from mip_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything, we need to load the data. The EMSData class reads and preprocesses the data for usage in simulation and MIP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning patients to hospitals: 100%|██████████| 297700/297700 [00:52<00:00, 5669.71it/s]\n",
      "Computing travel times: 100%|██████████| 13994200/13994200 [01:06<00:00, 211712.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Skip this cell if you already have ems_data.pkl and don't need to regenerate with different parameters\n",
    "ems_data = EMSData(region_id=1, x_intervals=10, y_intervals=10, verbose=True)\n",
    "ems_data.save_instance('ems_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# stations: 46\n",
      "# demand nodes: 67\n"
     ]
    }
   ],
   "source": [
    "ems_data = EMSData.load_instance('ems_data.pkl')\n",
    "\n",
    "n_stations = len(ems_data.stations)\n",
    "n_demand_nodes = len(ems_data.demand_nodes)\n",
    "demand = ems_data.demand_nodes.demand\n",
    "print(f\"# stations: {n_stations}\")\n",
    "print(f\"# demand nodes: {n_demand_nodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "The Simulation class pulls relevant data from an EMSData instance and runs the simulation. Simulations are used to evaluate a solution (i.e., the number of ambulances at each station) as well as generate a dataset for the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickled Simulation instance is used by HTCondor jobs\n",
    "#sim = Simulation(data=ems_data, avg_calls_per_day=TORONTO_AVG_CALLS_PER_DAY, n_days=100, n_replications=10)\n",
    "sim = Simulation(data=ems_data, avg_calls_per_day=1000, n_days=100, n_replications=10)\n",
    "sim.save_instance('simulation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9508725812351208"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run multiple replications and evaluate coverage\n",
    "def evaluate_solution(solution):\n",
    "    sim_result = sim.run(solution)\n",
    "    result = sim_result.sum()  # Sum over replications\n",
    "    result = np.array([result[f'covered{i}']/result[f'total{i}'] for i in range(n_demand_nodes)])\n",
    "    result = np.nan_to_num(result, nan=0.0)\n",
    "    coverage = demand@result / demand.sum()  # Estimated long-term coverage\n",
    "    return coverage\n",
    "\n",
    "# Evaluate the solution that places 5 ambulances at each station\n",
    "evaluate_solution([5]*n_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9513710152281083"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the solution that has unlimited ambulances at each station (1000 is basically unlimited)\n",
    "evaluate_solution([1000]*n_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was generated using HTCondor and is stored in `dataset.csv`. `n_jobs = 100` jobs are run, each job performs the simulation for `solutions_per_job = 1000` solutions, and `n_replications = 10` replications are ran per solution. The resulting dataset has `n_jobs * solutions_per_job` samples, one per solution (the `n_replications` replications for a solution are aggregated into a single sample).\n",
    "\n",
    "To generate `dataset.csv`:\n",
    "1. Run `htcondor_setup.py` to generate `settings<Process>.csv` files. These files contain the solutions to be simulated on each HTCondor job.\n",
    "2. Move the following files to the HTCondor submit server:\n",
    "    - `simulation.sub`\n",
    "    - `run_job.sh`\n",
    "    - `run_job.py`\n",
    "    - `simulation.py`\n",
    "    - `settings$(Process).csv` for each `Process`\n",
    "    - `simulation.pkl`\n",
    "    - `sim-env.tar.gz` (see https://chtc.cs.wisc.edu/uw-research-computing/conda-installation, Option 1; the environment must have numpy and pandas)\n",
    "\n",
    "    The last two files go to your Squid directory (see https://chtc.cs.wisc.edu/uw-research-computing/file-avail-squid).\n",
    "    \n",
    "3. Run `condor_submit simulation.sub`. Once the jobs are done, move the `results<Process>.csv` files to a new folder named `sim_results`. Then run `create_dataset.py` to create the dataset from the `results<Process>.csv` files. For each solution, the script sums the `covered<i>` and `total<i>` columns over the replications and defines `coverage<i>` as the ratio of the two sums. The resulting dataset has columns `station<i>` for `i` in `range(n_stations)`, and `coverage<i>` for `i` in `range(n_demand_nodes)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station0</th>\n",
       "      <th>station1</th>\n",
       "      <th>station2</th>\n",
       "      <th>station3</th>\n",
       "      <th>station4</th>\n",
       "      <th>station5</th>\n",
       "      <th>station6</th>\n",
       "      <th>station7</th>\n",
       "      <th>station8</th>\n",
       "      <th>station9</th>\n",
       "      <th>...</th>\n",
       "      <th>coverage57</th>\n",
       "      <th>coverage58</th>\n",
       "      <th>coverage59</th>\n",
       "      <th>coverage60</th>\n",
       "      <th>coverage61</th>\n",
       "      <th>coverage62</th>\n",
       "      <th>coverage63</th>\n",
       "      <th>coverage64</th>\n",
       "      <th>coverage65</th>\n",
       "      <th>coverage66</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927025</td>\n",
       "      <td>0.905187</td>\n",
       "      <td>0.919772</td>\n",
       "      <td>0.868613</td>\n",
       "      <td>0.969797</td>\n",
       "      <td>0.914167</td>\n",
       "      <td>0.955526</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893645</td>\n",
       "      <td>0.902134</td>\n",
       "      <td>0.923345</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.834316</td>\n",
       "      <td>0.858432</td>\n",
       "      <td>0.923382</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915328</td>\n",
       "      <td>0.906570</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.895652</td>\n",
       "      <td>0.964265</td>\n",
       "      <td>0.903403</td>\n",
       "      <td>0.920114</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930064</td>\n",
       "      <td>0.912834</td>\n",
       "      <td>0.924211</td>\n",
       "      <td>0.919118</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.906962</td>\n",
       "      <td>0.955671</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931480</td>\n",
       "      <td>0.908427</td>\n",
       "      <td>0.924083</td>\n",
       "      <td>0.886525</td>\n",
       "      <td>0.970378</td>\n",
       "      <td>0.901681</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.773585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.926924</td>\n",
       "      <td>0.893367</td>\n",
       "      <td>0.928029</td>\n",
       "      <td>0.924370</td>\n",
       "      <td>0.956327</td>\n",
       "      <td>0.908037</td>\n",
       "      <td>0.947090</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.804716</td>\n",
       "      <td>0.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925392</td>\n",
       "      <td>0.914516</td>\n",
       "      <td>0.932284</td>\n",
       "      <td>0.872881</td>\n",
       "      <td>0.956100</td>\n",
       "      <td>0.914417</td>\n",
       "      <td>0.940415</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924575</td>\n",
       "      <td>0.901637</td>\n",
       "      <td>0.927235</td>\n",
       "      <td>0.867257</td>\n",
       "      <td>0.964025</td>\n",
       "      <td>0.908904</td>\n",
       "      <td>0.944518</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922358</td>\n",
       "      <td>0.907812</td>\n",
       "      <td>0.928703</td>\n",
       "      <td>0.942623</td>\n",
       "      <td>0.971351</td>\n",
       "      <td>0.901662</td>\n",
       "      <td>0.907552</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927244</td>\n",
       "      <td>0.916501</td>\n",
       "      <td>0.929034</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.908928</td>\n",
       "      <td>0.955086</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       station0  station1  station2  station3  station4  station5  station6  \\\n",
       "0            12         7         1         1         2         0         3   \n",
       "1             7        14         2         6         2         6        11   \n",
       "2             7         4         3        19         5         3         6   \n",
       "3             4         2        25         5         1         5         1   \n",
       "4             4         5         0         5         3        18         9   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "99995         2         7         9         0         1         1        12   \n",
       "99996         3        16         5         8         5         9        11   \n",
       "99997         0         1         1         0         5        10         2   \n",
       "99998         1         3         4        12         9         4         8   \n",
       "99999         2        11         4         1         2        15        22   \n",
       "\n",
       "       station7  station8  station9  ...  coverage57  coverage58  coverage59  \\\n",
       "0             0         4         0  ...    0.927025    0.905187    0.919772   \n",
       "1             6        10         3  ...    0.893645    0.902134    0.923345   \n",
       "2            11         4         1  ...    0.915328    0.906570    0.924791   \n",
       "3             4         3         3  ...    0.930064    0.912834    0.924211   \n",
       "4             3         1        11  ...    0.931480    0.908427    0.924083   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "99995        14         0         2  ...    0.926924    0.893367    0.928029   \n",
       "99996         1         3        18  ...    0.925392    0.914516    0.932284   \n",
       "99997         8         2         0  ...    0.924575    0.901637    0.927235   \n",
       "99998         3         2         6  ...    0.922358    0.907812    0.928703   \n",
       "99999         8         3         4  ...    0.927244    0.916501    0.929034   \n",
       "\n",
       "       coverage60  coverage61  coverage62  coverage63  coverage64  coverage65  \\\n",
       "0        0.868613    0.969797    0.914167    0.955526    0.781250    0.666667   \n",
       "1        0.925000    0.834316    0.858432    0.923382    0.718750    1.000000   \n",
       "2        0.895652    0.964265    0.903403    0.920114    0.727273    0.666667   \n",
       "3        0.919118    0.958333    0.906962    0.955671    0.693878    1.000000   \n",
       "4        0.886525    0.970378    0.901681    0.904494    0.617647    1.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "99995    0.924370    0.956327    0.908037    0.947090    0.862069    0.804716   \n",
       "99996    0.872881    0.956100    0.914417    0.940415    0.750000    1.000000   \n",
       "99997    0.867257    0.964025    0.908904    0.944518    0.756757    0.600000   \n",
       "99998    0.942623    0.971351    0.901662    0.907552    0.666667    1.000000   \n",
       "99999    0.931818    0.968750    0.908928    0.955086    0.700000    1.000000   \n",
       "\n",
       "       coverage66  \n",
       "0        0.857143  \n",
       "1        0.704545  \n",
       "2        0.647059  \n",
       "3        0.700000  \n",
       "4        0.773585  \n",
       "...           ...  \n",
       "99995    0.853659  \n",
       "99996    0.725000  \n",
       "99997    0.777778  \n",
       "99998    0.861111  \n",
       "99999    0.666667  \n",
       "\n",
       "[100000 rows x 113 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "The MLP takes as input the solution and outputs the coverage probabilities for each demand node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 46]),\n",
       " torch.Size([25000, 46]),\n",
       " torch.Size([25000, 46]),\n",
       " torch.Size([50000, 67]),\n",
       " torch.Size([25000, 67]),\n",
       " torch.Size([25000, 67]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move dataset into tensors compatible with model and split into train/dev/test sets\n",
    "X = dataset[[f'station{i}' for i in range(n_stations)]].to_numpy()\n",
    "y = dataset[[f'coverage{i}' for i in range(n_demand_nodes)]].to_numpy()\n",
    "X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "y = torch.tensor(y, dtype=torch.float32, device=device)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.75)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, random_state=0, train_size=2/3)\n",
    "X_train.shape, X_dev.shape, X_test.shape, y_train.shape, y_dev.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 391/391 [00:00<00:00, 520.51it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 809.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train loss: 18439157788.18048, initial dev loss: 18420646394.59328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 1/100): 100%|██████████| 391/391 [00:01<00:00, 299.37it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 750.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 142479357.7836, dev loss: 1559577.81246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 2/100): 100%|██████████| 391/391 [00:00<00:00, 423.62it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 725.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1643754.1594, dev loss: 1090298.64112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 3/100): 100%|██████████| 391/391 [00:00<00:00, 418.25it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 748.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1241354.85076, dev loss: 936709.64406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 4/100): 100%|██████████| 391/391 [00:00<00:00, 420.88it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 762.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1053192.22758, dev loss: 940387.63106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 5/100): 100%|██████████| 391/391 [00:00<00:00, 413.62it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 723.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 940882.62454, dev loss: 863859.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 6/100): 100%|██████████| 391/391 [00:00<00:00, 417.73it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 748.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 870062.79732, dev loss: 984263.65074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 7/100): 100%|██████████| 391/391 [00:00<00:00, 393.76it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 673.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 817486.70786, dev loss: 797917.39794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 8/100): 100%|██████████| 391/391 [00:00<00:00, 393.76it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 668.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 785568.7216, dev loss: 892948.11738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 9/100): 100%|██████████| 391/391 [00:00<00:00, 392.18it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 642.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 756565.72476, dev loss: 854094.15598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 10/100): 100%|██████████| 391/391 [00:00<00:00, 391.00it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 636.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 737144.0611, dev loss: 796842.94414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 11/100): 100%|██████████| 391/391 [00:01<00:00, 370.27it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 624.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 718586.04328, dev loss: 751107.33922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 12/100): 100%|██████████| 391/391 [00:01<00:00, 381.46it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 632.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 702088.21846, dev loss: 739569.1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 13/100): 100%|██████████| 391/391 [00:01<00:00, 382.58it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 640.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 689890.0252, dev loss: 828118.21092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 14/100): 100%|██████████| 391/391 [00:01<00:00, 384.47it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 616.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 673857.72966, dev loss: 707137.65082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 15/100): 100%|██████████| 391/391 [00:01<00:00, 373.81it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 618.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 666382.55766, dev loss: 747193.84278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 16/100): 100%|██████████| 391/391 [00:01<00:00, 382.58it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 624.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 647744.70812, dev loss: 739703.24688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 17/100): 100%|██████████| 391/391 [00:01<00:00, 385.98it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 630.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 638642.64104, dev loss: 720684.17766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 18/100): 100%|██████████| 391/391 [00:01<00:00, 376.32it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 624.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 623112.16578, dev loss: 821675.91368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 19/100): 100%|██████████| 391/391 [00:01<00:00, 382.58it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 620.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 613291.03322, dev loss: 1100074.17326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 20/100): 100%|██████████| 391/391 [00:01<00:00, 382.21it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 632.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 596614.35216, dev loss: 632667.33753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 21/100): 100%|██████████| 391/391 [00:01<00:00, 384.09it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 630.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 575460.58406, dev loss: 774452.89376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 22/100): 100%|██████████| 391/391 [00:01<00:00, 362.04it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 620.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 561170.76056, dev loss: 648324.5144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 23/100): 100%|██████████| 391/391 [00:01<00:00, 377.05it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 626.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 547071.57278, dev loss: 638169.21199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 24/100): 100%|██████████| 391/391 [00:01<00:00, 381.09it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 618.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 528089.97418, dev loss: 818615.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 25/100): 100%|██████████| 391/391 [00:01<00:00, 387.13it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 624.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 501123.3061, dev loss: 594060.70952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 26/100): 100%|██████████| 391/391 [00:01<00:00, 376.69it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 608.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 485453.83615, dev loss: 470143.21491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 27/100): 100%|██████████| 391/391 [00:01<00:00, 379.24it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 618.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 464982.65839, dev loss: 630285.65488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 28/100): 100%|██████████| 391/391 [00:01<00:00, 374.52it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 614.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 440539.03379, dev loss: 518726.31673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 29/100): 100%|██████████| 391/391 [00:01<00:00, 375.96it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 632.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 417539.48059, dev loss: 529234.49318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 30/100): 100%|██████████| 391/391 [00:01<00:00, 377.41it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 628.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 397008.15088, dev loss: 365190.56595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 31/100): 100%|██████████| 391/391 [00:01<00:00, 380.56it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 614.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 384656.42452, dev loss: 479400.64998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 32/100): 100%|██████████| 391/391 [00:01<00:00, 381.46it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 628.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 366265.08031, dev loss: 458925.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 33/100): 100%|██████████| 391/391 [00:01<00:00, 379.98it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 628.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 348261.42879, dev loss: 527925.05326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 34/100): 100%|██████████| 391/391 [00:01<00:00, 376.70it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 620.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 337297.33393, dev loss: 495905.73146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 35/100): 100%|██████████| 391/391 [00:01<00:00, 378.51it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 620.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 327048.63365, dev loss: 343490.56568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 36/100): 100%|██████████| 391/391 [00:01<00:00, 375.96it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 617.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 310799.30208, dev loss: 513013.73466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 37/100): 100%|██████████| 391/391 [00:01<00:00, 379.24it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 612.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 297107.79993, dev loss: 431133.23794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 38/100): 100%|██████████| 391/391 [00:01<00:00, 374.86it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 625.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 289034.78254, dev loss: 419305.92194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 39/100): 100%|██████████| 391/391 [00:01<00:00, 382.96it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 626.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 274721.76307, dev loss: 634832.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 40/100): 100%|██████████| 391/391 [00:01<00:00, 383.71it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 618.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 256271.92617, dev loss: 428003.20168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 41/100): 100%|██████████| 391/391 [00:01<00:00, 379.24it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 620.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 246027.07866, dev loss: 351552.72415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 42/100): 100%|██████████| 391/391 [00:01<00:00, 381.46it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 634.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 235966.141955, dev loss: 304255.27651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 43/100): 100%|██████████| 391/391 [00:01<00:00, 382.96it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 603.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 224728.551295, dev loss: 379641.15407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 44/100): 100%|██████████| 391/391 [00:01<00:00, 380.35it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 638.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 214011.88887, dev loss: 294864.42044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 45/100): 100%|██████████| 391/391 [00:01<00:00, 383.33it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 628.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 201170.85749, dev loss: 207777.052725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 46/100): 100%|██████████| 391/391 [00:01<00:00, 378.14it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 618.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 194210.961375, dev loss: 410481.75291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 47/100): 100%|██████████| 391/391 [00:01<00:00, 375.60it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 628.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 186405.499415, dev loss: 350043.98936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 48/100): 100%|██████████| 391/391 [00:01<00:00, 381.84it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 622.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 177590.82276, dev loss: 232261.859965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 49/100): 100%|██████████| 391/391 [00:01<00:00, 379.98it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 622.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 167966.262755, dev loss: 232261.87099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 50/100): 100%|██████████| 391/391 [00:01<00:00, 378.51it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 630.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 165169.01483, dev loss: 305552.14342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 51/100): 100%|██████████| 391/391 [00:01<00:00, 379.61it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 614.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 156925.8765, dev loss: 217188.34267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 52/100): 100%|██████████| 391/391 [00:01<00:00, 383.71it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 632.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 154622.41432, dev loss: 268319.02743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 53/100): 100%|██████████| 391/391 [00:01<00:00, 382.21it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 638.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 148855.5422825, dev loss: 370615.00579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 54/100): 100%|██████████| 391/391 [00:01<00:00, 385.60it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 618.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 146046.67472, dev loss: 458124.10082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 55/100): 100%|██████████| 391/391 [00:01<00:00, 385.22it/s]\n",
      "Evaluating: 100%|██████████| 196/196 [00:00<00:00, 638.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 141400.636175, dev loss: 515183.58158\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train MLP\n",
    "weights = torch.tensor(demand, dtype=torch.float32, device=device)\n",
    "loss_fn = lambda logits, targets: MLP.regression_loss(logits, targets, weights, sum_outputs=True, modified_sigmoid=True)\n",
    "\n",
    "mlp = MLP(n_stations, [200], n_demand_nodes, dropout=0.1).to(device)\n",
    "init_train_loss = mlp.evaluate_loss(X_train, y_train, loss_fn)\n",
    "init_dev_loss = mlp.evaluate_loss(X_dev, y_dev, loss_fn)\n",
    "print(f\"Initial train loss: {init_train_loss}, initial dev loss: {init_dev_loss}\")\n",
    "mlp.fit(X_train, y_train, X_dev, y_dev, loss_fn, tolerance=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (0): Linear(in_features=46, out_features=200, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.0, inplace=False)\n",
       "  (3): Linear(in_features=200, out_features=67, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a saved model\n",
    "mlp = MLP.load_model('model.pt').to(device)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "The trained MLP is embedded within a MIP which attempts to find the solution that the MLP predicts will have the highest coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-11-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 60\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: AMD Ryzen 5 3600 6-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 12 logical processors, using up to 12 threads\n",
      "\n",
      "Optimize a model with 68 rows, 15724 columns and 16702 nonzeros\n",
      "Model fingerprint: 0x32c83fdc\n",
      "Variable types: 0 continuous, 15724 integer (15678 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [4e-71, 1e+04]\n",
      "  Bounds range     [1e+00, 2e+02]\n",
      "  RHS range        [2e+02, 2e+02]\n",
      "Found heuristic solution: objective -0.0000000\n",
      "Presolve removed 0 rows and 12807 columns\n",
      "Presolve time: 0.02s\n",
      "Presolved: 68 rows, 2917 columns, 3687 nonzeros\n",
      "Variable types: 0 continuous, 2917 integer (2883 binary)\n",
      "Found heuristic solution: objective 108718.00000\n",
      "\n",
      "Root relaxation: objective 2.977000e+05, 176 iterations, 0.01 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 297700.000    0   19 108718.000 297700.000   174%     -    0s\n",
      "H    0     0                    297700.00000 297700.000  0.00%     -    0s\n",
      "     0     0 297700.000    0   19 297700.000 297700.000  0.00%     -    0s\n",
      "\n",
      "Explored 1 nodes (178 simplex iterations) in 0.06 seconds (0.02 work units)\n",
      "Thread count was 12 (of 12 available processors)\n",
      "\n",
      "Solution count 3: 297700 108718 -0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.976999999992e+05, best bound 2.976999999992e+05, gap 0.0000%\n",
      "MEXCLP solution: [2, 4, 0, 1, 0, 10, 0, 3, 1, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 0, 79, 0, 0, 25, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 36, 0, 28, 0, 0, 0]\n",
      "Total ambulances used: 234\n",
      "Long-term coverage of MEXCLP: 0.8543732335482729\n"
     ]
    }
   ],
   "source": [
    "# Daskin's MEXCLP model\n",
    "coverage = compute_coverage(ems_data)\n",
    "solution = mexclp(demand=demand, coverage=coverage, n_ambulances=TORONTO_N_AMBULANCES, busy_fraction=0.5, time_limit=60, verbose=True)\n",
    "print(f\"MEXCLP solution: {solution}\\nTotal ambulances used: {sum(solution)}\")\n",
    "score = evaluate_solution(solution)\n",
    "print(f\"Long-term coverage of MEXCLP: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 60\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: AMD Ryzen 5 3600 6-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 12 logical processors, using up to 12 threads\n",
      "\n",
      "Optimize a model with 1651 rows, 826 columns and 25879 nonzeros\n",
      "Model fingerprint: 0xd715233b\n",
      "Variable types: 580 continuous, 246 integer (200 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-06, 4e+02]\n",
      "  Objective range  [1e+00, 2e+04]\n",
      "  Bounds range     [1e+00, 2e+02]\n",
      "  RHS range        [1e-05, 4e+02]\n",
      "Found heuristic solution: objective 157354.85726\n",
      "Presolve removed 46 rows and 46 columns\n",
      "Presolve time: 0.04s\n",
      "Presolved: 1605 rows, 780 columns, 25787 nonzeros\n",
      "Variable types: 534 continuous, 246 integer (200 binary)\n",
      "\n",
      "Root relaxation: objective 2.977000e+05, 1532 iterations, 0.08 seconds (0.15 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 297700.000    0   80 157354.857 297700.000  89.2%     -    0s\n",
      "     0     0 292698.089    0   68 157354.857 292698.089  86.0%     -    0s\n",
      "     0     0 292538.642    0   65 157354.857 292538.642  85.9%     -    1s\n",
      "     0     0 292538.316    0   67 157354.857 292538.316  85.9%     -    1s\n",
      "     0     0 292538.202    0   66 157354.857 292538.202  85.9%     -    1s\n",
      "     0     0 292538.202    0   66 157354.857 292538.202  85.9%     -    2s\n",
      "     0     2 292538.202    0   66 157354.857 292538.202  85.9%     -    2s\n",
      "*  644   712              90    283634.03494 292537.479  3.14%  28.8    4s\n",
      "*  686   712              89    283634.69220 292537.479  3.14%  29.8    4s\n",
      "H  854   774                    284535.23689 292537.418  2.81%  36.1    4s\n",
      "  1177  1351 285715.686   63   21 284535.237 292537.418  2.81%  41.9    5s\n",
      "* 1246  1317              75    284743.47503 292537.418  2.74%  40.5    5s\n",
      "  1531  1305 285752.677   32   68 284743.475 292536.171  2.74%  36.7   10s\n",
      "  1556  1328 290557.014   14   64 284743.475 292328.580  2.66%  40.3   15s\n",
      "H 2041  1444                    285533.32543 292328.580  2.38%  55.1   18s\n",
      "H 2138  1326                    285656.47027 292328.580  2.34%  54.5   18s\n",
      "* 2303  1251             102    285667.24755 292328.580  2.33%  57.5   19s\n",
      "  2431  1254 286626.148   22   48 285667.248 291093.522  1.90%  61.8   20s\n",
      "* 2952  1186             121    285789.24083 290001.640  1.47%  67.7   21s\n",
      "  4025  1079 286365.986   22   51 285789.241 289183.885  1.19%  85.9   25s\n",
      "  5676  1222     cutoff   24      285789.241 288518.048  0.95%   101   30s\n",
      "  7588  1509 286211.946   22   54 285789.241 287973.647  0.76%   110   35s\n",
      "  8816  1616 286195.492   20   59 285789.241 287684.718  0.66%   112   40s\n",
      " 11277  1708     cutoff   24      285789.241 287258.295  0.51%   116   45s\n",
      " 14179  1560     cutoff   24      285789.241 286533.407  0.26%   117   50s\n",
      "*15780  1932              86    285873.10914 286276.510  0.14%   111   52s\n",
      "*18698  2269              94    285883.06757 286097.329  0.07%  98.4   53s\n",
      "*18699  2255              94    285883.97060 286097.329  0.07%  98.4   53s\n",
      "*19460  2703              83    285886.18488 286093.537  0.07%  95.6   54s\n",
      "*19498  2585              96    285892.08003 286092.890  0.07%  95.5   54s\n",
      "*19574  2157              82    285915.05332 286092.890  0.06%  95.2   54s\n",
      " 20644  2490 285930.545   51   32 285915.053 286071.882  0.05%  92.0   55s\n",
      "*23056  2821              80    285920.65677 286053.218  0.05%  85.3   56s\n",
      " 28783  4489 285920.789   56   35 285920.657 286005.838  0.03%  73.9   60s\n",
      "*30582  4250              87    285925.55151 286005.568  0.03%  71.0   60s\n",
      "\n",
      "Cutting planes:\n",
      "  MIR: 44\n",
      "  Flow cover: 23\n",
      "  RLT: 9\n",
      "  Relax-and-lift: 1\n",
      "\n",
      "Explored 30967 nodes (2181496 simplex iterations) in 60.04 seconds (107.55 work units)\n",
      "Thread count was 12 (of 12 available processors)\n",
      "\n",
      "Solution count 10: 285926 285921 285915 ... 285667\n",
      "\n",
      "Time limit reached\n",
      "Best objective 2.859255515071e+05, best bound 2.860006593935e+05, gap 0.0263%\n",
      "Our model's solution: [0, 0, 7, 1, 0, 2, 0, 20, 2, 0, 1, 1, 18, 1, 1, 5, 4, 0, 2, 0, 3, 3, 0, 2, 17, 3, 54, 1, 1, 6, 2, 0, 0, 15, 0, 2, 4, 2, 4, 0, 2, 1, 1, 0, 0, 0]\n",
      "Total ambulances used: 188\n",
      "Long-term coverage of our model: 0.9136650757732502\n"
     ]
    }
   ],
   "source": [
    "# Our model with MLP embedded\n",
    "solution = mexclp_mlp(demand=demand, mlp=mlp, n_ambulances=TORONTO_N_AMBULANCES, time_limit=60, verbose=True)\n",
    "print(f\"Our model's solution: {solution}\\nTotal ambulances used: {sum(solution)}\")\n",
    "score = evaluate_solution(solution)\n",
    "print(f\"Long-term coverage of our model: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter TimeLimit to value 60\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (win64)\n",
      "\n",
      "CPU model: AMD Ryzen 5 3600 6-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 6 physical cores, 12 logical processors, using up to 12 threads\n",
      "\n",
      "Optimize a model with 1651 rows, 826 columns and 25879 nonzeros\n",
      "Model fingerprint: 0x1c5a39ab\n",
      "Variable types: 580 continuous, 246 integer (200 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-06, 2e+03]\n",
      "  Objective range  [1e+00, 2e+04]\n",
      "  Bounds range     [1e+00, 1e+03]\n",
      "  RHS range        [1e-05, 2e+03]\n",
      "Found heuristic solution: objective 157354.85726\n",
      "Presolve removed 46 rows and 46 columns\n",
      "Presolve time: 0.04s\n",
      "Presolved: 1605 rows, 780 columns, 25787 nonzeros\n",
      "Variable types: 534 continuous, 246 integer (200 binary)\n",
      "\n",
      "Root relaxation: objective 2.977000e+05, 678 iterations, 0.02 seconds (0.04 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0 297700.000    0   52 157354.857 297700.000  89.2%     -    0s\n",
      "     0     0 297700.000    0   72 157354.857 297700.000  89.2%     -    0s\n",
      "     0     0 296708.920    0   62 157354.857 296708.920  88.6%     -    1s\n",
      "     0     0 296320.285    0   60 157354.857 296320.285  88.3%     -    1s\n",
      "     0     0 296319.955    0   60 157354.857 296319.955  88.3%     -    1s\n",
      "     0     0 295665.946    0   60 157354.857 295665.946  87.9%     -    2s\n",
      "     0     0 295665.946    0   60 157354.857 295665.946  87.9%     -    2s\n",
      "H    0     0                    255484.35540 295665.946  15.7%     -    3s\n",
      "     0     0 295665.323    0   60 255484.355 295665.323  15.7%     -    3s\n",
      "     0     2 295665.323    0   60 255484.355 295665.323  15.7%     -    3s\n",
      "*  647   555              85    285291.65422 295665.323  3.64%  61.4    4s\n",
      "*  693   498              85    285493.43082 295665.323  3.56%  58.1    4s\n",
      "   794   520 287174.830   16   43 285493.431 293957.069  2.96%  66.7    5s\n",
      "* 1141   763              97    285494.14933 293957.069  2.96%  58.9    5s\n",
      "* 1145   758             100    285499.54272 293957.069  2.96%  58.7    5s\n",
      "* 1150   756              98    285500.83964 293957.069  2.96%  58.4    5s\n",
      "* 1202   687             133    285621.22096 293957.069  2.92%  58.6    5s\n",
      "  1273   694 285641.064   44   61 285621.221 293931.233  2.91%  60.6   10s\n",
      "  1322   734 289750.644   21   56 285621.221 293931.233  2.91%  67.6   15s\n",
      "* 2129   917             130    285762.89378 293931.233  2.86%  77.6   17s\n",
      "* 2134   779             117    285842.51318 293931.233  2.83%  77.5   17s\n",
      "  3014   603 287165.127   23   53 285842.513 288731.774  1.01%  97.5   20s\n",
      "  5323   665     cutoff   37      285842.513 287175.908  0.47%   111   25s\n",
      "  8874  1398 286096.151   38   45 285842.513 286178.949  0.12%  95.6   31s\n",
      "* 9082  1396              92    285843.59755 286177.368  0.12%  94.2   31s\n",
      "*10327  1851              93    285849.53556 286146.760  0.10%  87.6   31s\n",
      "*10334  1845              93    285849.72949 286146.760  0.10%  87.5   31s\n",
      "*11201  2165             105    285880.07500 286136.711  0.09%  83.1   32s\n",
      " 14651  3663 285901.692   46   40 285880.075 286081.927  0.07%  71.9   35s\n",
      " 23630  6380 285886.914   67   23 285880.075 286032.832  0.05%  57.4   40s\n",
      "*27460  7452              96    285880.38548 286021.835  0.05%  54.1   43s\n",
      "*27461  7439              96    285880.54983 286021.835  0.05%  54.0   43s\n",
      "*27464  7425              95    285880.80937 286021.835  0.05%  54.0   43s\n",
      " 29517  8077 285993.716   53   39 285880.809 286016.488  0.05%  52.5   45s\n",
      "*31272  8157              94    285883.98463 286013.394  0.05%  51.4   45s\n",
      "*31279  8114              95    285884.71312 286013.394  0.05%  51.4   45s\n",
      "*32873  8420              95    285891.29296 286009.018  0.04%  50.4   47s\n",
      " 37690  9467 285938.231   55   39 285891.293 285998.766  0.04%  47.9   50s\n",
      "*37758  9452             104    285891.39889 285998.766  0.04%  47.9   50s\n",
      "*37803  7649              90    285908.89601 285998.766  0.03%  47.8   50s\n",
      "*43357  7069              92    285916.90630 285987.827  0.02%  45.7   53s\n",
      " 43948  7093     cutoff   46      285916.906 285985.667  0.02%  45.4   57s\n",
      "H43976  7091                    285916.91421 285985.667  0.02%  45.4   57s\n",
      "*45409  7200              81    285917.99726 285984.721  0.02%  44.8   58s\n",
      "*46145  7296              90    285918.13540 285982.406  0.02%  44.4   59s\n",
      "*47191  6986              84    285920.59974 285981.110  0.02%  44.1   59s\n",
      " 48083  7067 285924.262   58   28 285920.600 285979.282  0.02%  43.7   60s\n",
      "*48198  4190              90    285940.68795 285979.282  0.01%  43.7   60s\n",
      "*48203  4078              91    285941.60807 285979.282  0.01%  43.7   60s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 1\n",
      "  MIR: 40\n",
      "  Flow cover: 14\n",
      "  RLT: 18\n",
      "\n",
      "Explored 48577 nodes (2117174 simplex iterations) in 60.03 seconds (108.49 work units)\n",
      "Thread count was 12 (of 12 available processors)\n",
      "\n",
      "Solution count 10: 285942 285941 285921 ... 285891\n",
      "\n",
      "Time limit reached\n",
      "Best objective 2.859416080699e+05, best bound 2.859792819496e+05, gap 0.0132%\n",
      "Our model's solution: [0, 0, 6, 2, 0, 2, 0, 17, 2, 0, 2, 1, 22, 3, 2, 6, 3, 0, 2, 0, 1, 3, 0, 2, 15, 2, 56, 2, 1, 10, 2, 0, 0, 13, 0, 2, 5, 2, 4, 0, 3, 2, 2, 0, 0, 0]\n",
      "Total ambulances used: 197\n",
      "Long-term coverage of our model: 0.9197291448929594\n"
     ]
    }
   ],
   "source": [
    "# Our model with MLP embedded, n_ambulances=1000 to see if model makes use of all ambulances\n",
    "solution = mexclp_mlp(demand=demand, mlp=mlp, n_ambulances=1000, time_limit=60, verbose=True)\n",
    "print(f\"Our model's solution: {solution}\\nTotal ambulances used: {sum(solution)}\")\n",
    "score = evaluate_solution(solution)\n",
    "print(f\"Long-term coverage of our model: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that when the neural net is trained on samples each using exactly 234 total ambulances, increasing `n_ambulances` beyond 234 does not help us.\n",
    "- The function mapping solutions to overall coverage should be monotone: if $x \\le \\hat{x}$, then $\\hat{x}$ should perform at least as well as $x$. Does the neural net behave like this?\n",
    "- Some stations are simply better than others. Since all samples $(x, y)$ in the dataset have $x$ summing to 234, placing more ambulances at \"bad\" stations means taking ambulances away from \"good\" stations. Does this mean the neural net is actually learning which stations are bad, and if so, would increasing the input for a bad station reduce the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this research is to compete with other optimization models for the ambulance location problem, such as Daskin's MEXCLP. We want to show the disconnect between other models (specifically their objective function) and reality (i.e., simulation).\n",
    "- What we are NOT trying to do: use ML to solve MEXCLP.\n",
    "- What we ARE trying to do: use ML to model reality (i.e., as a surrogate for the simulation).\n",
    "\n",
    "Other notes:\n",
    "- The Muskoka instance (8 (actually 5, original code produced 8) stations, 62 demand nodes, 30 ambulances) may not be that interesting: only around 10M solutions, and the neural net doesn't reveal any solutions that significantly outperform the best simulation solution. The best simulation solution and our model's solution are comparable and outperform Daskin's MEXCLP; since stations are evenly spaced, the assumptions Daskin makes are reasonable, so outperforming MEXCLP is a big win.\n",
    "- We can attack the problem from either the ML side (improving the neural net, e.g., architecture, loss function, training) or the MIP side. Is there something akin to regularization but for the MIP model?\n",
    "    - Use MEXCLP's objective as a feature to the ML model?\n",
    "- Applications to queueing?\n",
    "- Decision-aware learning/smart predict then optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
